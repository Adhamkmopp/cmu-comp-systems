## 6.22
Assuming that x is from 0 to 1 then:

I.  The number of bits per track is expressed as the innermost circumfrance: 2.x.r.<&pi>.R where R is the recording density
II. The number of tracks on a platter is: r.(1-x).T where T is the track density
III.    The total area then is the product of both: (2.x.r.<&pi>.R).(r.(1-x).T)

Without knowing what the values of R or T are, I cannot determine the exact value of x that will min-max the equation

## 6.23
T<sub>avg rotation</sub>= (1/2) * (60/12,000) * 1000 ms/sec
T<sub>avg transfer</sub>= (60/12,000) * 1/500 * 1000 ms/sec
T<sub>access</sub> = 3 + T<sub>avg rotation</sub> + T<sub>avg transfer</sub>


## 6.24
The 2MB file is split across 2048 logical blocks/sectors. In the best case, the time required to read the file is: 

T<sub>avg seek</sub> + T<sub>avg rotation</sub> + T<sub>avg transfer</sub> * 2,048

T<sub>avg transfer</sub> is calculated as (60/18,000) * (1/2,000) * 1000 =  0.003 ms
and T<sub>avg rotation</sub> is (60/18,000)* (1/2) * (1,000) =  1.67 ms

So the total time is: 8 + 1.67 + (0.003) * 2,048 = 15.805 ms to read the full file sequentially.

In the worst case, the head would have to fly continously to different tracks to read a sector, for a total of 2,048 jumps:

The total time would be (T<sub>avg seek</sub> + T<sub>avg rotation</sub>)*2,048 + T<sub>avg transfer</sub> * 2,048 = 19 seconds



## 6.25:

Cache	m	C	B	E	S	t	s	b
1	32	1,024	4	4	64	24	6	2
2	32	1,024	4	256	1	30	0	2	* fully associative
3	32	1,024	8	1	128	22	7	3
4	32	1,024	8	128	1	29	0	3	* fully associative
5	32	1,024	32	1	32	22	5	5
6	32	1,024	32	4	16	21	4	5

## 6.26:

Cache	m	C	B	E	S	t	s	b
1	32	2,048	8	1	256	21	8	3
2	32	2,048	4	4	128	23	7	2
3	32	1,024	2	8	64	25	6	1
4	32	1.024	32	2	16	23	4	5



## 6.27:

Set 1: 

0100 0101 001 xx
0 1000 1010 01xx = 0x08A4 - 0x08A7
	           0x0704 - 0x0707
Set 6:

1001 0001 110 xx
1 0010 0011 10xx = 0x1238 - 0x123B
1 1110 0001 10xx = 0x1E18 - 0x1E1B

## 6.28:

A. None
B. 1100 0111 100 xx	0000 0101 100 xx
C. 0111 0001 101 xx
D. 1101 1110 111 xx

## 6.29:

A.

CO: 0 to 1
CI: 2 to 3
CT: 4 to 12

B.

operation	address	hit?	read value (or unknown)
read	0x834	n		1000 0011 0100
write	0x836	y	-	1000 0011 0110
read	0xffd	y	C0	1111 1111 1101

## 6.30:

C0: 0 to 1
CI: 2 to 4
CT: 5 to 13

cache size is 4 x 4 x 8 = 128 bytes

## 6.31:

A.
bits : 0011 1000 110 10

B.

parameter	value
block offset	0x2
index	0x6
cache tag	0x38
cache hit	y
cache byte returned	0xEB

## 6.32:

A.
bits : 1011 0111 010 00

B.

parameter	value
block offset	0x0
index	0x2
cache tag	0xb7
cache hit	n
cache byte returned	0xEB

## 6.33:

1 0111 1000 10xx = 0x1788 to 0x178B
1 0110 1100 10xx = 0x16C8 to 0x16CB

## 6.34:

typedef int array[4][4];

void transpose2 (array dst, array src){

	int i,j;
	for(i =0; i<4;i++){
		for(j=0; j<4; j++){
			dst[i][j]=src[i][j];
		}
	}
}

			dst					src
	col0	col1	col2	col3		col0	col1	col2	col3
row1	m	m	m	m		m	h	h	h
row2	m	m	m	m		m	h	h	h
row2	m	m	m	m		m	h	h	h
row2	m	m	m	m		m	h	h	h

## 6.35:

			dst					src
	col0	col1	col2	col3		col0	col1	col2	col3
row1	m	h	h	h		m	h	h	h
row2	m	h	h	h		m	h	h	h
row2	m	h	h	h		m	h	h	h
row2	m	h	h	h		m	h	h	h

## 6.36:


int x[2][128];
int i;
int sum = 0;

for (i = 0; i < 128; i++){
	sum += x[0][i] * x[1][i];
}

A. The total read count is 256, and the addresses of x[0][i] is 0 to 512 and x[1][i] is from 512 to 1024. There are 32 sets each 16 (4 ints) bytes long, then the miss rate would be 100% (continuous thrashing) since each read will be directed to an occupied set.
B. 1/4, as every fourth is a cold miss baby
C. 1/4 cold misses again bay-beeee
D. No. Cold as ice.
E. Yes, cold misses are reduced by half when the size is doubled.

## 6.37:

The cache has 256 sets, each 16 (4 ints) bytes long.
Array a starts at: 00001000000000000000 00000000 0000

Function	N = 64	N = 60
sumA	1/4	1/4
sumB	1	1/4
sumC	1/2	?

In sumB, there is a jump of 256 addresses (64 * 4) or 16 set indices for each iteration of the inner loop, meaning that row 17 will be a conflict miss with row 1 in set 0 until the loop is done. The outerloop then updates, repeating the same conflict misses as before.
For N = 60, there is  jump of 240 addresses (60 * 4) or 15 set indices for the inner loop such that row 17 will be in set 255. The sets restarts to set 14 with row 18 onwards and so on, making full use of the cache. After  the initial round of cold misses, there are 3 hits for each j from 1 to 3, then conflict misses and the pattern repeats.

For sumC, the situation is that the first and second addition operands hit to the same set, always, so an initial miss rate of 1 is reduced to 1/2. This seems like an oversimplification considering the boundry condition where a[i][j] is on the last black offset, and a[i][j+1] is a cache miss as it maps to a different set.


## 6.38:

A.	 16 x 16 x 4
B.	16 x 16 x 3
C.	3/4

## 6.40:

A.	16 x 16 x 4
B. & C.	The miss rate is 100% in the first loop, and only 512/768 = 75% in the second loop.

## 6.41:

On every iteration of the inner loop, an array element is accessed 640 elements away in memory (consequence of row order form), or the address accessed is 640 * 4 =2560 bytes away from the previous address. This corresponds to 2560/8 = 320 set alterations, meaning 80 cycles back to set 0, so the only miss rate is the consistent cold miss rate.

## 6.42:

Since the flow is now linear, and each cache line/set can hold up to 8 elements, then the cache miss rate is only the first cold miss for every set so 1/8

## 6.43:

The miss rate becomes half as each cache block can only hold two ints at a time


## 6.45:

This is an interesting case. I made 4 variations on top of the normal transpose function and these are the results from a single run:


Normal: 14108 clicks (0.014108 seconds).
Transpose by block: 4260 clicks (0.004260 seconds).
Loop unrolling: 6903 clicks (0.006903 seconds).
Aligned transpose + Loop unrolling: 4686 clicks (0.004686 seconds).
Transpose by block + Loop unrolling: 2749 clicks (0.002749 seconds).


Transposing by block was meant as a measure of reducing cache misses by transposing the matrix block by block. In theory this means that after the first row is read from the source and cache lines are filled with rows of the destination N elements apart, there will be an increased chance that elements from the second column will hit as the loop iterates over the second row of the source. Without blocking, there will be continious cache conflicts as the each row of the source is read and destination blocks are written to the cache. Given that this machine has an 8-way associative 32kb L1 cache with 64B lines, the assumption is that in the worst case, an 8x8 block will result in only one cache set filled with the entire working space for the next 8 iterations of the loop over the rows of the source. In the best case, a 512x512 block will result in the entire workspace filling every set and every line in the cache. Both scenarios are broken however since I am not accounting for the source cache conflicts and is therefore highly ideal.

After testing, a 16x16 block was found to be optimal if N = 1024. The conclusion is that even though transposing by block and loop unrolling was the fastest in this case, performing a memory aligned tranposed may be preferable as it is more robust; the performance varies greatly with tweaking the block size in relation to the number of elements of the matrix which may be beyond one's control.

What is most interesting out of all of these results, it that making sure that the starting address is properly aligned has lead to a signficant speedup comparable to tranposing by blocks, and could be useful to implement by itself if the need arises. It's quick and efficient.


## 6.46:

Fairly straight foward after 6.45:

Normal: 1219803 clicks (1.219803 seconds).
Symmetric + block: 779102 clicks (0.009984 seconds).

The differences are signficiant. The second version applied the principle of using blocks as before, while accounting for the symmetric nature of the adjacency matrix (one accumelator for each), and lastly, measures were put in place to make sure that all the elements were passed over once and only once with no redundancy.

I am still dissapointed that I could not crack the use the cache in a formal way to speed it (or 6.45) even more. I don't know if learning about caches and how they work is going to be very useful in the future.