"Determining which combinations of transformations to apply is indeed part of the "black art" of writing fast code."


## Optimization blockers:

Specifically for compilers, some code aspects block certain optimizations. These include:

I.  Memory aliases: if a compiler cannot determine whether or not pointers will be aliases/refer to the same location.
II. Function calls: repeated function calls are costly where a reduced number of  calls is not (bookkeeping: parameters, local variables, rbp and rsi are saved/set each time). A functional call may need repeated calls however, blocking inline substituion.


## Critical paths & Overhead: 

This deserves a section all by itself. Critical paths are chains of dependencies that slow down a program through repeated execution. This is only meaningful in the context of clock cycles spent per instruction, which in turn, is made clear after learning about the hardware of the cpu and what clocked registers and combinatorial circuits mean, and how they're updated. "Wasteful" time spent means that a clock cycle has been wasted, which generally means (aside from optimizing basic arithmetic functions) that a register was updated, or data was involved in a memory operation which otherwise could have been avoided. 


## Latency Bounds: Reducing Operations

Below is the code for allocating a vector and two more functions for retrieving elements and lengths which will be used in the subsequent 5 refinements over a code that performs some arithmetic operation on all elements of the vector.

```c++

typedef long data_t;

    typedef struct{
        long len;
        data_t *data;
    } vec_rec, *vec_ptr;


vec_ptr new_vec(long len){
    /* Allocate header structure */
    vec_ptr result = (vec_ptr) malloc(sizeof(vec_rec));
    data_t *data=NULL;
    if (!result)
        return NULL;    /* Could not allocate storage */
    result ->len=len;
    if(len>0)
        data = (data_t*)calloc(len, sizeof(data_t));
        if(!data){
            free((void*) result);
            return NULL;    /* Could not allocate storage */
        }
        result ->data = data;
        return result; 
}

int get_vec_element (vec_ptr v, long index, data_t *dest){
    if (index < 0 || index >= v->len)
        return 0;
    *dest = v ->data[index];
    return 1;
}

long vec_length(vec_ptr v){
    return v -> len;
}
```

### Pure Form

The first iteration of a sigma operation is slow, because it calls vec_length repeatedly each iteration of the loop with all that entails (possibly storing rbp and updating rsp).

```c++

void combine1 (vec_ptr v, data_t *dest){
    long i;
    *dest = 0;
    for (i = 0; i <vec_length(v); i++){
        data_t val;
        get_vec_element (v, i , &val);
        *dest = *dest + val;
    }
}
```


### Reducing Function Calls

The second iteration is much faster by comparison as it eliminates repeated function calls to vec_length.

```c++

void combine2 (vec_ptr v, data_t *dest){
    long i;
    long length = vec_length(v);
    *dest = 0;
    for (i = 0; i <length; i++){
        data_t val;
        get_vec_element (v, i , &val);
        *dest = *dest + val;
    }
}
```

### Reducing Function Calls (Again)

The third iteration does away with function calls entirely by setting a pointer to the start of the vector and using pointer arithmetic to fetch the next element (a load operation running in parallel to the multiplication/addition operation every clock cycle).

```c++
data_t *get_vec_start(vec_ptr v){
    return v ->data;
}

void combine3 (vec_ptr v, data_t *dest){
    long i;
    long length = vec_length(v);
    data_t *data = get_vec_start(v);
    
    *dest = 0;
    for (i = 0; i <length; i++){
        *dest = *dest + data[i];
    }
}
```

This should have theoretically led to much higher performance improvements, but it did not. There are other bottlenecks that are not yet apparent in code.

### Eliminating Memory References

The fourth iteration eliminates memory references by avoiding repeated calls to dereference *dest and storing the data in a seperate variable instead (to be stored in regsiter xmm0).
```c++
data_t *get_vec_start(vec_ptr v){
    return v ->data;
}

void combine4 (vec_ptr v, data_t *dest){
    long i;
    long length = vec_length(v);
    data_t *data = get_vec_start(v);
    data_t acc = 0;
    
    
    for (i = 0; i <length; i++){
        acc = acc + data[i];
    }
    *dest = acc;
}
```
```c++
data_t *get_vec_start(vec_ptr v){
    return v ->data;
}


### Reducing Overhead (Loop Unrolling)

Here, two or more elements are added at a time reducing overhead (which probably involves branch prediction and something else that happens within the loop aside from termination/initiation):

void combine5  (vec_ptr v, data_t *dest){

    long i;
    long length = vec_length(v);
    long limit = length -1;
    data_t *data = get_vec_start(v);
    data_t acc = 0;

    for (i = 0; i <limit; i+=2){
        acc = (acc + data[i]) + data[i+1];
    }

    for (; i < length; i++){
        acc = acc + data[i];
    }
    *dest=acc;
}

In terms of machine code, combine5 does a more direct memory reference:

i in %rdx
vmulsd  (%rax,%rdx,8), %xmm0, %xmm0
vmulsd  8(%rax,%rdx,8), %xmm0, %xmm0
addq    $2, %rdx

As opposed to combine4:

data[i] in %rdx
vmulsd  %rdx, %xmm0, %xmm0
addq $8, %rdx

There are n/k operations performed with an equal number of load operations which maintains a throughput bound on all operations, but the overhead is reduced by n/k


## Throughput Bounds: Exploiting Parallelism

Similar to loop unrolling, this method attemps to exploit the remaining number of functional units. The code below performs an operation on the odd and even indices of the vector in parallel.

void combine6  (vec_ptr v, data_t *dest){

    long i;
    long length = vec_length(v);
    long limit = length -1;
    data_t *data = get_vec_start(v);
    data_t acc0 = 0;
    data_t acc1 = 0;

    for (i = 0; i <limit; i+=2){
        acc0 = (acc0 + data[i]);
        acc1 = (acc1 + data[i+1]);
    }

    for (; i < length; i++){
        acc0 = acc0 + data[i];
    }
    *dest=acc0 + acc1;
}


*** One dangerous implication of this method if splitting the inputs leads to overflow, such as the case where the majority of elements are very high on even indices, and very low on odd indices and the operation is multiplication. Be mindful.

## Limitations

Two limitations are register spilling and branch prediction. Register spilling is where the number of available registers is exhausted and the CPU has to resort to memory loading and storing. The cure is to simply not assume that higher loop unrolling is automatically better and to be mindful. Branch prediciton may be fixed (if it is indeed an issue. It may not be.) by writing code that directly translates to a conditional move:


void minmax2 (long a[], long b[], long n){
    long i;
    for (i=0; i < n; i++){
        long min = a[i] < b[i] ? a[i] : b[i];
        long max = a[i] < b[i] ? b[i] : a[i];
        a[i] = min;
        b[i] = max;
    }
}

